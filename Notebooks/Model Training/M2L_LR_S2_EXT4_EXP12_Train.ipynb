{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzPASWLDPGA8",
        "outputId": "2fafe414-f98f-44b1-b925-3fa76480346d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPt9q_aDPU6H"
      },
      "outputs": [],
      "source": [
        "# !pip install \"peft==0.2.0\"\n",
        "# !pip install \"transformers==4.27.2\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"bitsandbytes==0.37.1\" loralib --upgrade --quiet\n",
        "\n",
        "#pip install peft transformers datasets accelerate evaluate bitsandbytes loralib --upgrade --quiet\n",
        "# install additional dependencies needed for training\n",
        "# !pip install rouge-score tensorboard py7zr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFOTm14fbmXr"
      },
      "outputs": [],
      "source": [
        "!pip install peft transformers datasets accelerate evaluate bitsandbytes loralib --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71TAePw8bmXs",
        "outputId": "e3169a06-ff7d-442e-d02e-24e0f6af3dd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggb1yeN7bmXs",
        "outputId": "b5291012-41b8-4ee4-91e8-4a1148607b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.62.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.5.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.25.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HBt-PwJueDc"
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk, concatenate_datasets, DatasetDict\n",
        "\n",
        "#textile patent documents\n",
        "dataset = DatasetDict({\n",
        "    'train': load_from_disk('/content/drive/MyDrive/FYP-Abstractive Text Summarization/Data/Textile_Patent_(70-20-10)_LexRank_thres_3/train'),\n",
        "    'validation': load_from_disk('/content/drive/MyDrive/FYP-Abstractive Text Summarization/Data/Textile_Patent_(70-20-10)_LexRank_thres_3/validation'),\n",
        "    'test': load_from_disk('/content/drive/MyDrive/FYP-Abstractive Text Summarization/Data/Textile_Patent_(70-20-10)_LexRank_thres_3/test')\n",
        "})\n",
        "#dataset = load_from_disk('/content/drive/MyDrive/FYP-Abstractive Text Summarization/Data/Textile_Patent_(70-20-10)_LexRank')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHukBiadg2VG",
        "outputId": "8aea8e93-865e-416c-e12a-244dbca3670f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 7905\n",
            "Validation dataset size: 1130\n",
            "Test dataset size: 2259\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
        "print(f\"Validation dataset size: {len(dataset['validation'])}\")\n",
        "print(f\"Test dataset size: {len(dataset['test'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiGK2ItShJU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d5e434-9ad8-49a7-8da9-4ce2a6ba856b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_id=\"facebook/bart-large\"\n",
        "\n",
        "# Load tokenizer of FLAN-t5-SMALL\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "#model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD7SBg1Kk7mw"
      },
      "outputs": [],
      "source": [
        "#len(dataset[\"train\"]['description'][4])\n",
        "# min_len = 9999999\n",
        "# for i in range(len(dataset['train'])):\n",
        "#   temp = len(dataset[\"train\"]['description'][i])\n",
        "#   if (temp < min_len):\n",
        "#     min_len = temp\n",
        "\n",
        "# print(min_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgMsyAiqhZNj"
      },
      "outputs": [],
      "source": [
        "# from datasets import concatenate_datasets\n",
        "# import numpy as np\n",
        "# # The maximum total input sequence length after tokenization.\n",
        "# # Sequences longer than this will be truncated, sequences shorter will be padded.\n",
        "# tokenized_inputs = dataset[\"train\"].map(lambda x: tokenizer(x[\"description\"], truncation=True), batched=True, remove_columns=[\"description\", \"abstract\"])\n",
        "# input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
        "# # take 85 percentile of max length for better utilization\n",
        "# max_source_length = int(np.percentile(input_lenghts, 85))\n",
        "# print(f\"Max source length: {max_source_length}\")\n",
        "\n",
        "# # The maximum total sequence length for target text after tokenization.\n",
        "# # Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
        "# tokenized_targets = dataset[\"train\"].map(lambda x: tokenizer(x[\"abstract\"], truncation=True), batched=True, remove_columns=[\"description\", \"abstract\"])\n",
        "# target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
        "# # take 90 percentile of max length for better utilization\n",
        "# max_target_length = int(np.percentile(target_lenghts, 90))\n",
        "# print(f\"Max target length: {max_target_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dEvyr8Ukmc0"
      },
      "outputs": [],
      "source": [
        "#Maximum available for bart model\n",
        "max_source_length = 1024\n",
        "max_target_length = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkO9ZbxCkWyj"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(sample,padding=\"max_length\"):\n",
        "    # add prefix to the input for bart\n",
        "    #inputs = [\"summarize: \" + extract_important_sentences(item) for item in sample[\"description\"]]\n",
        "    inputs = [\"summarize: \" + item for item in sample[\"description\"]]\n",
        "\n",
        "\n",
        "    # tokenize inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
        "\n",
        "    # Tokenize targets with the `text_target` keyword argument\n",
        "    labels = tokenizer(text_target=sample[\"abstract\"], max_length=max_target_length, padding=padding, truncation=True)\n",
        "\n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
        "    # padding in the loss.\n",
        "    if padding == \"max_length\":\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO7kxQBsxMAp"
      },
      "outputs": [],
      "source": [
        "# tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"description\", \"abstract\"])\n",
        "# print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6SBa1r0zpyW"
      },
      "outputs": [],
      "source": [
        "# # save datasets to disk for later easy loading\n",
        "# tokenized_dataset[\"train\"].save_to_disk(\"/content/drive/MyDrive/FYP-Abstractive Text Summarization/Adapter_Tuning/LoRA+Bart/tokenized_data_S2_EXT4_M2L/train\")\n",
        "# tokenized_dataset[\"validation\"].save_to_disk(\"/content/drive/MyDrive/FYP-Abstractive Text Summarization/Adapter_Tuning/LoRA+Bart/tokenized_data_S2_EXT4_M2L/validation\")\n",
        "# tokenized_dataset[\"test\"].save_to_disk(\"/content/drive/MyDrive/FYP-Abstractive Text Summarization/Adapter_Tuning/LoRA+Bart/tokenized_data_S2_EXT4_M2L/test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N4pQQirbmXv"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "tokenized_dataset = DatasetDict({\n",
        "    'train': load_from_disk('/content/drive/MyDrive/FYP-Abstractive Text Summarization/Adapter_Tuning/LoRA+Bart/tokenized_data_S2_EXT4_M2L/train'),\n",
        "    'validation': load_from_disk('/content/drive/MyDrive/FYP-Abstractive Text Summarization/Adapter_Tuning/LoRA+Bart/tokenized_data_S2_EXT4_M2L/validation'),\n",
        "    'test': load_from_disk('/content/drive/MyDrive/FYP-Abstractive Text Summarization/Adapter_Tuning/LoRA+Bart/tokenized_data_S2_EXT4_M2L/test')\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD6FYNrebmXv"
      },
      "outputs": [],
      "source": [
        "# tokenized_data = {\n",
        "#     'train': train,\n",
        "#     'validation': valid,\n",
        "#     'test': test\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g89867i6bmXv",
        "outputId": "b0869d33-dcd1-4ab4-98a8-30c3f97d4bde"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 7905\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 1130\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 2259\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GzlYWS4ZLQJ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "# huggingface hub model id\n",
        "model_id = \"facebook/bart-large\"\n",
        "\n",
        "# load model from the hub\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoNCMz0Ka9CB"
      },
      "outputs": [],
      "source": [
        "# !pip list | grep cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "135nRepMbmXw",
        "outputId": "27ed322b-6715-4eb9-edd0-cb2fbf2c52d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BartForConditionalGeneration(\n",
            "  (model): BartModel(\n",
            "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
            "    (encoder): BartEncoder(\n",
            "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
            "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
            "      (layers): ModuleList(\n",
            "        (0-11): 12 x BartEncoderLayer(\n",
            "          (self_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (activation_fn): GELUActivation()\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (decoder): BartDecoder(\n",
            "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
            "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
            "      (layers): ModuleList(\n",
            "        (0-11): 12 x BartDecoderLayer(\n",
            "          (self_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (activation_fn): GELUActivation()\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVB0SgmcbmXw"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# #check for gpu\n",
        "# if torch.backends.mps.is_available():\n",
        "#    mps_device = torch.device(\"mps\")\n",
        "#    x = torch.ones(1, device=mps_device)\n",
        "#    print (x)\n",
        "# else:\n",
        "#    print (\"MPS device not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOpqVwc9bmXw",
        "outputId": "cdcd589e-4eb2-4871-a34d-ffcacba989f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linux-6.1.58+-x86_64-with-glibc2.35\n"
          ]
        }
      ],
      "source": [
        "import platform\n",
        "print(platform.platform())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1J18UGPZRuj",
        "outputId": "1d50f549-2731-4c1b-d68c-9055bfb2c4a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,718,592 || all params: 411,010,048 || trainable%: 1.1480478452925802\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig\n",
        "from peft import get_peft_model\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from peft import TaskType\n",
        "# Define LoRA Config\n",
        "lora_config = LoraConfig(\n",
        " r=16,\n",
        " lora_alpha=128,\n",
        " target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
        " lora_dropout=0.05,\n",
        " bias=\"none\",\n",
        " task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "# prepare int-8 model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# add LoRA adaptor\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGgeHfzxcu7L"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "# we want to ignore tokenizer pad token in the loss\n",
        "label_pad_token_id = -100\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=label_pad_token_id,\n",
        "    pad_to_multiple_of=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHdOqkVBu8cy"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "output_dir=\"lora-bart-large\"\n",
        "\n",
        "# Define training args\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "\tauto_find_batch_size=True,\n",
        "    learning_rate=1e-3, # higher learning rate\n",
        "    num_train_epochs=5,\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=500,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['validation'],\n",
        "\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxZjEZXDbmXw"
      },
      "outputs": [],
      "source": [
        "# device = torch.device(\"mps:0\")\n",
        "# model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t33usFMbdamo",
        "outputId": "f05998f3-ac86-499a-90a6-5d40e65c6b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModelForSeq2SeqLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): BartForConditionalGeneration(\n",
            "      (model): BartModel(\n",
            "        (shared): Embedding(50265, 1024, padding_idx=1)\n",
            "        (encoder): BartEncoder(\n",
            "          (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
            "          (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
            "          (layers): ModuleList(\n",
            "            (0-11): 12 x BartEncoderLayer(\n",
            "              (self_attn): BartSdpaAttention(\n",
            "                (k_proj): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                )\n",
            "                (v_proj): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                )\n",
            "                (q_proj): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                )\n",
            "                (out_proj): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                )\n",
            "              )\n",
            "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (activation_fn): GELUActivation()\n",
            "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (decoder): BartDecoder(\n",
            "          (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
            "          (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
            "          (layers): ModuleList(\n",
            "            (0-11): 12 x BartDecoderLayer(\n",
            "              (self_attn): BartSdpaAttention(\n",
            "                (k_proj): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                )\n",
            "                (v_proj): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                )\n",
            "                (q_proj): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                )\n",
            "                (out_proj): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                )\n",
            "              )\n",
            "              (activation_fn): GELUActivation()\n",
            "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (encoder_attn): BartSdpaAttention(\n",
            "                (k_proj): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                )\n",
            "                (v_proj): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                )\n",
            "                (q_proj): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                )\n",
            "                (out_proj): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                )\n",
            "              )\n",
            "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "YA0yw-DWz4vZ",
        "outputId": "69f7d2fd-1c05-488c-f659-9456ffb216aa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4945' max='4945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4945/4945 1:08:20, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.897600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.491800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>3.688300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>3.562700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>3.453400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>3.392100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>3.308000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>3.259900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>3.183000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4945, training_loss=3.3418310151905333, metrics={'train_runtime': 4101.6397, 'train_samples_per_second': 9.636, 'train_steps_per_second': 1.206, 'total_flos': 8.6800679829504e+16, 'train_loss': 3.3418310151905333, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# train model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU8W9FBR0Cfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2442c3d0-2814-49b4-ed19-0400ceb5fa20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/FYP-Abstractive Text Summarization/Adapter_Tuning/LoRA+Bart/M2L_LR_S2_EX4_EXP12_tokenizer/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/FYP-Abstractive Text Summarization/Adapter_Tuning/LoRA+Bart/M2L_LR_S2_EX4_EXP12_tokenizer/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/FYP-Abstractive Text Summarization/Adapter_Tuning/LoRA+Bart/M2L_LR_S2_EX4_EXP12_tokenizer/vocab.json',\n",
              " '/content/drive/MyDrive/FYP-Abstractive Text Summarization/Adapter_Tuning/LoRA+Bart/M2L_LR_S2_EX4_EXP12_tokenizer/merges.txt',\n",
              " '/content/drive/MyDrive/FYP-Abstractive Text Summarization/Adapter_Tuning/LoRA+Bart/M2L_LR_S2_EX4_EXP12_tokenizer/added_tokens.json',\n",
              " '/content/drive/MyDrive/FYP-Abstractive Text Summarization/Adapter_Tuning/LoRA+Bart/M2L_LR_S2_EX4_EXP12_tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Save our LoRA model & tokenizer results\n",
        "peft_model_id=\"M2L_LR_S2_EX4_EXP12\"\n",
        "trainer.model.save_pretrained(f'/content/drive/MyDrive/FYP-Abstractive Text Summarization/Adapter_Tuning/LoRA+Bart/{peft_model_id}_model')\n",
        "tokenizer.save_pretrained(f'/content/drive/MyDrive/FYP-Abstractive Text Summarization/Adapter_Tuning/LoRA+Bart/{peft_model_id}_tokenizer')\n",
        "# if you want to save the base model to call\n",
        "# trainer.model.base_model.save_pretrained(peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O59Ma1V37kpc"
      },
      "outputs": [],
      "source": [
        "test_dataset = load_from_disk(\"/content/drive/MyDrive/FYP-Abstractive Text Summarization/Adapter_Tuning/LoRA+T5/tokenized_data/test\").with_format(\"torch\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHPSnT6O5MRb"
      },
      "outputs": [],
      "source": [
        "for sample in test_dataset:\n",
        " input = tokenizer.decode(sample['input_ids'].detach().cpu().numpy(), skip_special_tokens=True)\n",
        " output = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)\n",
        " prediction = tokenizer.decode(output[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
        " break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "OH44GoBt6Zr2",
        "outputId": "5cc1b535-852f-467e-e144-22320e3c5114"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'summarize: CROSS-REFERENCE TO RELATED APPLICATIONS The present application claims priority from Australian Provisional Patent Application No 2006903417 filed on 26 Jun. 2006, the content of which is incorporated herein by reference. INTRODUCTION TO THE INVENTION This invention relates to the packaging of consumer products and relates particularly to products that are formed of pulp material, particularly pulp material formed of waste paper, waste timber, waste fabric material, virgin timber and other similar pulp material. The invention more particularly relates to high quality products carrying high definition printing or other decoration. BACKGROUND TO THE INVENTION The creation of compelling and high quality packaging for consumer durables is well established and is executed in a variety of forms and formats known in the prior art but with each of the prior art, the formats and methodologies having their own particular limitations. The consumer market demands increasing colour, vibrancy and novelty in addition to sophistication in order to provide eye catching shapes that will serve to differentiate products available for sale in a given marketplace. An element of protection is also required for the goods in question and such protection requirements of the packaging in question, often require complex internal structures or substructures to protect the product in question which introduces, in some cases, considerable cost to the packaging products commonly available. The core packaging functions, that is to contain, protect, preserve and promote the products in question, are often offset by substantial cost and lack of sustainability, that is the material is from a non-renewable source, or manufactured with a process that causes harmful environmental emissions, or in such a way as to preclude recycling and re-use. The cost of packaging can add considerably to the final cost of a product as it enters commerce and it is desirable to provide the best packaging possible at the most economical cost. Sustainability is also another key issue and becoming an increasingly politicised issue of keen interest in the minds of consumers who may consider the type of packaging used for a product as part of any buying decision. In addition, there is a general move and sympathy towards the provision of legislation and guidelines against non-sustainable packaging of consumer products. The commonly available packaging techniques and materials can be summarised as follows: Paper and Cardboard Paper or Cardboard packaging is the most common form of packaging found in the market today. Paper and cardboard packaging is low cost and has the ability to accept printing and finishing'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "cF2lxTTz5Es-",
        "outputId": "182dd93a-1c6f-4bf9-dbc1-19d3ada63eaa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The present invention relates to product packaging which comprises high quality products carrying high definition printing and/or other similar products in the manufacture and/or any product that is able to be recycled.'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbuQybEt5YzJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}