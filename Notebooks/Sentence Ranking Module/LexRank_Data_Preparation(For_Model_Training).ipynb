{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nevidujayatilleke/Documents/FYP-Abstractive Text Summarization/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/nevidujayatilleke/Documents/FYP-Abstractive Text Summarization/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets, Dataset, DatasetDict\n",
    "\n",
    "#textile patent documents\n",
    "dataset = load_from_disk('../../Data/Textile_Patents_(70-20-10)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 7905\n",
      "Validation dataset size: 1130\n",
      "Test dataset size: 2259\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Validation dataset size: {len(dataset['validation'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['description', 'abstract'],\n",
       "        num_rows: 7905\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['description', 'abstract'],\n",
       "        num_rows: 1130\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['description', 'abstract'],\n",
       "        num_rows: 2259\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nevidujayatilleke/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nevidujayatilleke/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sentence1, sentence2):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence1 = [w for w in sentence1 if w not in stop_words]\n",
    "    filtered_sentence2 = [w for w in sentence2 if w not in stop_words]\n",
    "    all_words = list(set(filtered_sentence1 + filtered_sentence2))\n",
    "    vector1 = [filtered_sentence1.count(word) for word in all_words]\n",
    "    vector2 = [filtered_sentence2.count(word) for word in all_words]\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(sentences):\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                similarity_matrix[i][j] = sentence_similarity(sentences[i], sentences[j])\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lexrank(similarity_matrix, damping=0.85, threshold=0.2, max_iter=100):\n",
    "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "    scores = nx.pagerank(nx_graph, alpha=damping, tol=threshold, max_iter=max_iter)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sentences(sentences, scores):\n",
    "    ranked_sentences = sorted(((scores[i], sentence) for i, sentence in enumerate(sentences)), reverse=True)\n",
    "    top_sentences = [sentence for score, sentence in ranked_sentences]\n",
    "    return top_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_important_sentences(text):\n",
    "    preprocessed_sentences = preprocess_text(text)\n",
    "    similarity_matrix = build_similarity_matrix(preprocessed_sentences)\n",
    "    scores = apply_lexrank(similarity_matrix)\n",
    "    top_sentences = get_top_sentences(preprocessed_sentences, scores)\n",
    "    paragraph = ' '.join([' '.join(sentence) for sentence in top_sentences])\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_text_lexrank(row):\n",
    "    #inputs = [\"summarize: \" + extract_important_sentences(item) for item in sample[\"description\"]]\n",
    "    row['description'] = extract_important_sentences(row['description'])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function update_text_lexrank at 0x176df6700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|██████████| 7905/7905 [16:21:26<00:00,  7.45s/ examples]      \n",
      "Map: 100%|██████████| 1130/1130 [44:08<00:00,  2.34s/ examples] \n",
      "Map: 100%|██████████| 2259/2259 [5:54:56<00:00,  9.43s/ examples]     \n"
     ]
    }
   ],
   "source": [
    "processed_dataset = DatasetDict({\n",
    "    split_name: dataset.map(update_text_lexrank)\n",
    "    for split_name, dataset in dataset.items()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['description', 'abstract'],\n",
       "        num_rows: 7905\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['description', 'abstract'],\n",
       "        num_rows: 1130\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['description', 'abstract'],\n",
       "        num_rows: 2259\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 7905/7905 [00:00<00:00, 19110.27 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1130/1130 [00:00<00:00, 23759.00 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2259/2259 [00:00<00:00, 24438.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# save datasets to disk for later easy loading\n",
    "processed_dataset[\"train\"].save_to_disk(\"../../Data/Textile_Patent_(70-20-10)_LexRank_thres_3/train\")\n",
    "processed_dataset[\"validation\"].save_to_disk(\"../../Data/Textile_Patent_(70-20-10)_LexRank_thres_3/validation\")\n",
    "processed_dataset[\"test\"].save_to_disk(\"../../Data/Textile_Patent_(70-20-10)_LexRank_thres_3/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if desired , part of the hydrolysate can be re...</td>\n",
       "      <td>Processes for preparing pulp from lignin-conta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no . in one embodiment of the present inventio...</td>\n",
       "      <td>A durable erosion control blanket featuring a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no . [ 0025 ] in a further preferred embodimen...</td>\n",
       "      <td>A method for spinning a multifilament yarn fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no . no . no . no . a . a . repair . fig . fig...</td>\n",
       "      <td>A surgical repair device having a length to wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fig . fig . in fig . fig . fig . thereafter , ...</td>\n",
       "      <td>A transporting carriage for conveying a coiler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>as seen in the foregoing , according to the fu...</td>\n",
       "      <td>A clothes washing machine includes a main wash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>viewing the arrangements of connecting webs in...</td>\n",
       "      <td>A compressible cheese center for dyeing purpos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>1 and which is , therefore , the yarn which wi...</td>\n",
       "      <td>A yarn feeder for a circular knitting machine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2257</th>\n",
       "      <td>1 , in the embodiment of fig . at the outlet e...</td>\n",
       "      <td>A method and an apparatus for stuffer box crim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>no . no . no . 1 , 4 , 6 , and 8 . in fig . fi...</td>\n",
       "      <td>In a knitting machine, electromagnetically con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2259 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            description  \\\n",
       "0     if desired , part of the hydrolysate can be re...   \n",
       "1     no . in one embodiment of the present inventio...   \n",
       "2     no . [ 0025 ] in a further preferred embodimen...   \n",
       "3     no . no . no . no . a . a . repair . fig . fig...   \n",
       "4     fig . fig . in fig . fig . fig . thereafter , ...   \n",
       "...                                                 ...   \n",
       "2254  as seen in the foregoing , according to the fu...   \n",
       "2255  viewing the arrangements of connecting webs in...   \n",
       "2256  1 and which is , therefore , the yarn which wi...   \n",
       "2257  1 , in the embodiment of fig . at the outlet e...   \n",
       "2258  no . no . no . 1 , 4 , 6 , and 8 . in fig . fi...   \n",
       "\n",
       "                                               abstract  \n",
       "0     Processes for preparing pulp from lignin-conta...  \n",
       "1     A durable erosion control blanket featuring a ...  \n",
       "2     A method for spinning a multifilament yarn fro...  \n",
       "3     A surgical repair device having a length to wi...  \n",
       "4     A transporting carriage for conveying a coiler...  \n",
       "...                                                 ...  \n",
       "2254  A clothes washing machine includes a main wash...  \n",
       "2255  A compressible cheese center for dyeing purpos...  \n",
       "2256  A yarn feeder for a circular knitting machine ...  \n",
       "2257  A method and an apparatus for stuffer box crim...  \n",
       "2258  In a knitting machine, electromagnetically con...  \n",
       "\n",
       "[2259 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
